{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ajfisch/deeplearning_bootcamp_2020/blob/master/advanced_vision_tutorial_2020.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GA8LLAO9dchw"
      },
      "source": [
        "# PyTorch Basics \n",
        "\n",
        "In this tutorial, we'll develop our dataset and model with PyTorch\n",
        "\n",
        "Let's get started!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "eT_jLzv8do9z"
      },
      "outputs": [],
      "source": [
        "# http://pytorch.org/\n",
        "from os import path\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Gives a progress bar\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "from torch.utils import data\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "print(torch.__version__)\n",
        "print(torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CONSTANTS\n",
        "RADII = [2,3]\n",
        "NUM_SAMPLES = 10000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "s2jGlFlidci-"
      },
      "source": [
        "## Step 1: Building the dataset\n",
        "\n",
        "Datasets are abstractions that hold data for you. As long as you define a `__len__` and `__getitem__`, they can be used to pipe data into your training routine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ConcentricCircles(data.Dataset):\n",
        "    def __init__(\n",
        "        self, \n",
        "        split,\n",
        "        radii = RADII, \n",
        "        num_samples = NUM_SAMPLES, \n",
        "        partition_ratio = [0.7, 0.15, 0.15],\n",
        "        seed = 0) -> None:\n",
        "        super(ConcentricCircles, self).__init__()\n",
        "\n",
        "        torch.random.manual_seed(seed)\n",
        "        X = 2 * (radii[-1] + 1) * torch.rand(num_samples,2) - (radii[-1] + 1)\n",
        "        Y = torch.zeros(num_samples)\n",
        "\n",
        "        if len(radii) == 1:\n",
        "            Y[ torch.where( (X**2).sum(-1)  > radii[0]**2 ) ] = 1\n",
        "        else:\n",
        "            for i, r in enumerate(radii):\n",
        "                Y[ torch.where( (X**2).sum(-1)  > r**2) ] = i+1\n",
        "\n",
        "\n",
        "        Y = Y.long()\n",
        "\n",
        "        shuffled_indices = torch.randperm(num_samples)\n",
        "        train_indx = shuffled_indices[:int(partition_ratio[0]*num_samples)]\n",
        "        val_indx = shuffled_indices[int(partition_ratio[0]*num_samples):  int(num_samples * sum(partition_ratio[:2]) ) ]\n",
        "        test_indx = shuffled_indices[int(num_samples * sum(partition_ratio[:2]) ): ]\n",
        "        if split == \"train\":\n",
        "            self.dataset = list(zip(X[train_indx], Y[train_indx]))\n",
        "        elif split == \"dev\": \n",
        "            self.dataset = list(zip(X[val_indx], Y[val_indx]))\n",
        "        elif split == \"test\":\n",
        "            self.dataset = list(zip(X[test_indx], Y[test_indx]))\n",
        "        \n",
        "        self.X, self.Y = X,Y\n",
        "        self.num_classes = 2 if len(radii) == 1 else len(radii) + 1\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.dataset[index] \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_circles(dataset):\n",
        "    x = torch.stack([x[0] for x in dataset])\n",
        "    y = [x[1] for x in dataset]\n",
        "    plt.scatter(x[:,0], x[:,1], c = y)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dataset = ConcentricCircles(split=\"train\")\n",
        "dev_dataset = ConcentricCircles(split=\"dev\")\n",
        "test_dataset = ConcentricCircles(split=\"test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# visualize data\n",
        "visualize_circles(train_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "s2jGlFlidci-"
      },
      "source": [
        "## Step 2: Building a model\n",
        "\n",
        "All pytorch models should be implemented as instances of `nn.Module`. \n",
        "\n",
        "To build a model you need to:\n",
        "\n",
        "1. define what parameters it'll need in it's `__init__` function\n",
        "\n",
        "2. define the model's computation, using those parameters, in the `forward` function.\n",
        "\n",
        "\n",
        "To keep things simple, lets define a simple linear classifer, like logistic regression. We'll experiment with more complex models soon."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Model(nn.Module):\n",
        "   \n",
        "    def __init__(self, num_classes):\n",
        "        super(Model, self).__init__()\n",
        "        self.fully_connected = nn.Linear(2, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fully_connected(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "M9hvStGzdcjC"
      },
      "source": [
        "## Step 3. Defining our training procedure\n",
        "\n",
        "To train our model, let's introduce a couple new PyTorch ideas.\n",
        "\n",
        "A [DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) is an iterator that goes over our entire dataset and selects batches. \n",
        "We'll be using this to iterate through our train/dev/test sets.\n",
        "\n",
        "Let's intialize these now. \n",
        "\n",
        "An [Optimizer](https://pytorch.org/docs/stable/optim.html) defines an update rule. In class, we've discussed vanilla SGD, which is one method to compute the next weight, given the current weight and gradient. There are plently of other optimizers you can try from the pytorch library. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "n3Ht1mhhdcjE"
      },
      "outputs": [],
      "source": [
        "# Training settings\n",
        "epochs = 10\n",
        "lr = .01\n",
        "momentum = 0.5\n",
        "batch_size = 32\n",
        "num_classes = train_dataset.num_classes\n",
        "\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "dev_loader = torch.utils.data.DataLoader(dev_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "model = Model(num_classes)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To train our model:\n",
        "\n",
        "1) we'll randomly sample batches from our train loader\n",
        "\n",
        "2) compute our loss (using standard `cross_entropy`)\n",
        "\n",
        "3) compute our gradients (by calling `backward()` on our loss)\n",
        "\n",
        "4) update our neural network with an `optimizer.step()`, and go back to 1)\n",
        "\n",
        "I've added some extra stuff here to log our accuracy and average loss for the epoch.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "wxIj0eWsdcjJ"
      },
      "outputs": [],
      "source": [
        "def train_epoch( model, train_loader, optimizer, epoch):\n",
        "    model.train() # Set the nn.Module to train mode. \n",
        "    model = model.to('cuda')\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    num_samples = len(train_loader.dataset)\n",
        "    for batch_idx, (x, target) in tqdm(enumerate(train_loader), total=len(train_loader)): #1) get batch\n",
        "        x, target = x.to('cuda'), target.to('cuda')\n",
        "        # Reset gradient data to 0\n",
        "        optimizer.zero_grad()\n",
        "        # Get prediction for batch\n",
        "        output = model(x)\n",
        "        # 2) Compute loss\n",
        "        loss = F.cross_entropy(output, target)\n",
        "        #3) Do backprop\n",
        "        loss.backward()\n",
        "        #4) Update model\n",
        "        optimizer.step()\n",
        "        \n",
        "        ## Do book-keeping to track accuracy and avg loss\n",
        "        pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "        total_loss += loss.detach() # Don't keep computation graph \n",
        "\n",
        "    print('Train Epoch: {} \\tLoss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
        "            epoch, total_loss / num_samples, \n",
        "            correct, \n",
        "            num_samples,\n",
        "            100. * correct / num_samples))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xGZnlI58dcjN"
      },
      "source": [
        "## Step 3.5 Define our evaluation loop\n",
        "Similar to above, we'll also loop through our dev or test set, and compute our loss and accuracy. \n",
        "This lets us see how well our model is generalizing. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "GPzqSuY3dcjO"
      },
      "outputs": [],
      "source": [
        "def eval_epoch(model, test_loader, name):\n",
        "    model = model.to('cuda')\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    for data, target in tqdm(test_loader):\n",
        "        data, target = data.to('cuda'), target.to('cuda')\n",
        "        output = model(data)\n",
        "        test_loss += F.cross_entropy(output, target).item() # sum up batch loss\n",
        "        pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print('\\n{} set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        name,\n",
        "        test_loss, \n",
        "        correct, \n",
        "        len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Pp9H5tWEdcjR"
      },
      "source": [
        "## Step 4: Training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "vVpU_N0idcjS"
      },
      "outputs": [],
      "source": [
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train_epoch(model, train_loader, optimizer, epoch)\n",
        "    eval_epoch(model,  dev_loader, \"Dev\")\n",
        "    print(\"---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WwBmuGetdcjY"
      },
      "source": [
        "# Step 5. Experiment with MLP\n",
        "Let's try a more complex model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "e4x3E0AcdcjZ"
      },
      "outputs": [],
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(Model, self).__init__()\n",
        "        self.fc1 = nn.Linear(2, 10)\n",
        "        self.fc2 = nn.Linear(10,10)\n",
        "        self.fc3 = nn.Linear(10, num_classes)\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "        hidden = F.relu(self.fc1(x))\n",
        "        hidden = F.relu(self.fc2(hidden))\n",
        "        logit = self.fc3(hidden)\n",
        "        return logit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = Model(num_classes)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train_epoch(model, train_loader, optimizer, epoch)\n",
        "    eval_epoch(model,  dev_loader, \"Dev\")\n",
        "    print(\"---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 6. Evaluate the best model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "dNWqKY72dcjs"
      },
      "outputs": [],
      "source": [
        "eval_epoch(model,  test_loader, \"Test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Saving and Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.save(model,'path.pt')     # to save\n",
        "model = torch.load('path.pt')   # to load a saved model"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "advanced_vision_tutorial_2020.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.1 64-bit ('3.9.1')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.1"
    },
    "vscode": {
      "interpreter": {
        "hash": "6e1c477bbb4741057bb356a8a4cfea3fd85ab06b5d35fee50af9f87046f40a78"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
